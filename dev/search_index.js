var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API Documentation","title":"API","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"CurrentModule = K8sClusterManagers","category":"page"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"K8sClusterManager\nisk8s","category":"page"},{"location":"api/#K8sClusterManagers.K8sClusterManager","page":"API Documentation","title":"K8sClusterManagers.K8sClusterManager","text":"K8sClusterManager(np::Integer; kwargs...)\n\nA cluster manager using Kubernetes (k8s) which spawns additional pods as workers. Attempts to spawn np workers but may launch with less workers if the cluster has less resources available.\n\nArguments\n\nnp: Desired number of worker pods to be launched.\n\nKeywords\n\nnamespace: the Kubernetes namespace to launch worker pods within. Defaults to current_namespace().\nmanager_pod_name: the name of the manager pod. Defaults to gethostname() which is the name of the pod when executed inside of a Kubernetes pod.\nworker_prefix: the prefix given to spawned workers. Defaults to \"$(manager_pod_name)-worker\" when the manager is running inside of K8s otherwise defaults to \"fv-az842-406-worker.\nimage: Docker image to use for the workers. Defaults to the image used by the manager when running inside of a K8s pod otherwise defaults to \"julia:VERSION\".\ncpu: CPU resources requested for each worker. Defaults to 1,\nmemory: Memory resource requested for each worker in bytes. Requests may provide a unit suffix (e.g. \"G\" for Gigabytes and \"Gi\" for Gibibytes). Defaults to \"4Gi\".\npending_timeout: The maximum number of seconds to wait for a \"Pending\" worker pod to enter the \"Running\" phase. Once the timeout has been reached the manager will continue with the number of workers available (<= np). Defaults to 180 (3 minutes).\nconfigure: A function which allows modification of the worker pod specification before their creation. Defaults to identity.\n\n\n\n\n\n","category":"type"},{"location":"api/#K8sClusterManagers.isk8s","page":"API Documentation","title":"K8sClusterManagers.isk8s","text":"isk8s() -> Bool\n\nPredicate for testing if the current process is running within a Kubernetes (K8s) pod.\n\n\n\n\n\n","category":"function"},{"location":"patterns/#Workflow-Patterns","page":"Workflow Patterns","title":"Workflow Patterns","text":"","category":"section"},{"location":"patterns/#Required-Permissions","page":"Workflow Patterns","title":"Required Permissions","text":"","category":"section"},{"location":"patterns/","page":"Workflow Patterns","title":"Workflow Patterns","text":"K8sClusterManagers.jl requires a minimal set of permisisons for managing worker Pods within the cluster. A minimal set of permissions is documented below along with a ServiceAccount to make use of these permissions:","category":"page"},{"location":"patterns/","page":"Workflow Patterns","title":"Workflow Patterns","text":"using Markdown\nMarkdown.parse(\"\"\"\n```yaml\n$(read(\"julia-manager-serviceaccount.yaml\", String))\n```\n\"\"\")","category":"page"},{"location":"patterns/#Use-K8sClusterManager-only-within-cluster","page":"Workflow Patterns","title":"Use K8sClusterManager only within cluster","text":"","category":"section"},{"location":"patterns/","page":"Workflow Patterns","title":"Workflow Patterns","text":"Since the K8sClusterManager can only be used when running inside of a Kubernetes Pod you may want conditionally use it. The isk8s predicate provides a convenient way of determining if the running Julia process is executing within a K8s Pod.","category":"page"},{"location":"patterns/","page":"Workflow Patterns","title":"Workflow Patterns","text":"using Distributed, K8sClusterManagers\n\nmanager = if isk8s()\n    K8sClusterManager(n)\nelse\n    Distributed.LocalManager(n)\nend\n\naddprocs(manager; exeflags=\"--project\")","category":"page"},{"location":"patterns/#Executing-a-script","page":"Workflow Patterns","title":"Executing a script","text":"","category":"section"},{"location":"patterns/","page":"Workflow Patterns","title":"Workflow Patterns","text":"Depending on your use case you may find yourself wanting to execute a script on the K8s cluster. One basic workflow would be as follows.","category":"page"},{"location":"patterns/","page":"Workflow Patterns","title":"Workflow Patterns","text":"Write a \"script.jl\" which uses K8sClusterManager\nBuild and push a Docker image containing the \"script.jl\" and the required dependencies:\ndocker build -t $IMAGE .\ndocker push $IMAGE\nDefine a Kubernetes manifest (\"script-example.template.yaml\") which executes the Docker image on the cluster. Note that the use of envsubst will substitute ${...} with the respectively named environmental variable.\napiVersion: v1\nkind: Pod\nmetadata:\n  generateName: script-example-\nspec:\n  serviceAccountName: \"${PROJECT}-service-account\"\n  restartPolicy: Never\n  containers:\n  - name: manager\n    image: \"${IMAGE}\"\n    imagePullPolicy: Always\n    command: [\"julia\", \"script.jl\"]\n    args: [\"${ARG}\"]\nCreate the resource which will run our script.\n# Expects that `PROJECT`, `IMAGE`, and `ARG` are all predefined\ncat script-example.template.yaml | envsubst | kubectl create -f -","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"The K8sClusterManager can be used both inside and outside of a Kubernetes cluster.","category":"page"},{"location":"examples/#Launching-an-interactive-session","page":"Examples","title":"Launching an interactive session","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"The following manifest will create a Kubernetes Job named \"interactive-session\". This Job will spawn a Pod (see spec.template.spec) which will run an interactive Julia session with the latest release of K8sClusterManagers.jl installed. Be sure to create the required ServiceAccount and associated permissions before proceeding.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Markdown\nMarkdown.parse(\"\"\"\n```yaml\n$(read(\"interactive-session.yaml\", String))\n```\n\"\"\")","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"To start the Job and attach to the interactive Julia session you can run the following:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Executed from the K8sClusterManager.jl root directory\nkubectl apply -f docs/src/interactive-session.yaml\n\n# Determine the generated pod name from the job\nmanager_pod=$(kubectl get pods -l job-name=interactive-session --output=jsonpath='{.items[*].metadata.name}')\necho $manager_pod\n\n# Attach to the interactive Julia session running in the pod.\n# Note: You may need to wait for K8sClusterManagers.jl to finish installing\nkubectl attach -it pod/${manager_pod?}","category":"page"},{"location":"examples/#Launching-workers","page":"Examples","title":"Launching workers","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"You can use K8sClusterManager to spawn workers within the K8s cluster. The cluster manager can be used both inside/outside of the K8s cluster. In the following example we'll be using a small amount of CPU/Memory to ensure workers can be spawned even on clusters with limited resources:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"julia> using Distributed, K8sClusterManagers\n\njulia> addprocs(K8sClusterManager(3, cpu=0.2, memory=\"300Mi\", pending_timeout=30))\n[ Info: interactive-example-sp28h-worker-7dvpt is up\n[ Info: interactive-example-sp28h-worker-vvrwm is up\n[ Info: interactive-example-sp28h-worker-bczm5 is up\n3-element Vector{Int64}:\n 2\n 3\n 4\n\njulia> pmap(x -> myid(), 1:nworkers())  # Each worker reports its worker ID\n3-element Vector{Int64}:\n 3\n 4\n 2","category":"page"},{"location":"examples/#Pending-workers","page":"Examples","title":"Pending workers","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"A worker created via addprocs may not necessarily be available right away, as K8s must schedule the worker's Pod to a Node before the corresponding Julia process can start. If K8s can't schedule the worker's Pod to a Node right away (e.g. not enough resources are available), then that Pod will sit around in the \"Pending\" phase until it can be scheduled. Since a worker Pod may be stuck in this \"Pending\" phase for an indefinite amount of time, K8sClusterManager includes the pending_timeout keyword that may used to specify how long you are willing to wait for pending workers. Once this timeout has been reached, the manager will continue with the subset of workers which have reported in and delete any workers that are stuck in the \"Pending\" phase.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"julia> addprocs(K8sClusterManager(1, memory=\"1Pi\", pending_timeout=10))  # Request 1 pebibyte of memory\n┌ Warning: TimeoutException: timed out after waiting for worker interactive-session-d7jfb-worker-ffvnm to start for 10 seconds, with status:\n│ {\n│     \"conditions\": [\n│         {\n│             \"lastProbeTime\": null,\n│             \"lastTransitionTime\": \"2021-05-04T19:07:19Z\",\n│             \"message\": \"0/1 nodes are available: 1 Insufficient memory.\",\n│             \"reason\": \"Unschedulable\",\n│             \"status\": \"False\",\n│             \"type\": \"PodScheduled\"\n│         }\n│     ],\n│     \"phase\": \"Pending\",\n│     \"qosClass\": \"Guaranteed\"\n│ }\n└ @ K8sClusterManagers ~/.julia/dev/K8sClusterManagers/src/native_driver.jl:113\nInt64[]","category":"page"},{"location":"examples/#Termination-Reason","page":"Examples","title":"Termination Reason","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"When Julia workers exceed the specified memory limit the worker Pod will be automatically killed by Kubernetes (OOMKilled). In such a scenario the worker will be reported as terminated by Distributed.jl without details. K8sClusterManagers.jl will provide the reason, as reported by K8s, for the termination of the worker:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"julia> @everywhere begin\n           function oom(T=Int64)\n               max_elements = Sys.total_memory() ÷ sizeof(T)\n               fill(zero(T), max_elements + 1)\n           end\n       end\n\njulia> remotecall_fetch(oom, last(workers()))\nWorker 4 terminated.ERROR:\nProcessExitedException(4)\nStacktrace:\n  [1] try_yieldto(undo::typeof(Base.ensure_rescheduled))\n    @ Base ./task.jl:705\n  [2] wait\n    @ ./task.jl:764 [inlined]\n  [3] wait(c::Base.GenericCondition{ReentrantLock})\n    @ Base ./condition.jl:106\n  [4] take_buffered(c::Channel{Any})\n    @ Base ./channels.jl:389\n  [5] take!(c::Channel{Any})\n    @ Base ./channels.jl:383\n  [6] take!(::Distributed.RemoteValue)\n    @ Distributed /buildworker/worker/package_linuxaarch64/build/usr/share/julia/stdlib/v1.6/Distributed/src/remotecall.jl:599\n  [7] #remotecall_fetch#143\n    @ /buildworker/worker/package_linuxaarch64/build/usr/share/julia/stdlib/v1.6/Distributed/src/remotecall.jl:390 [inlined]\n  [8] remotecall_fetch(::Function, ::Distributed.Worker)\n    @ Distributed /buildworker/worker/package_linuxaarch64/build/usr/share/julia/stdlib/v1.6/Distributed/src/remotecall.jl:386\n  [9] remotecall_fetch(::Function, ::Int64; kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})\n    @ Distributed /buildworker/worker/package_linuxaarch64/build/usr/share/julia/stdlib/v1.6/Distributed/src/remotecall.jl:421\n [10] remotecall_fetch(::Function, ::Int64)\n    @ Distributed /buildworker/worker/package_linuxaarch64/build/usr/share/julia/stdlib/v1.6/Distributed/src/remotecall.jl:421\n [11] top-level scope\n    @ REPL[4]:1\n\njulia> ┌ Warning: Worker 4 on pod interactive-session-nfz6j-worker-2hzsd was terminated due to: OOMKilled\n└ @ K8sClusterManagers ~/.julia/packages/K8sClusterManagers/hUL5i/src/native_driver.jl:171","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Markdown\nMarkdown.parse(\"\"\"\n$(read(joinpath(\"..\", \"..\", \"README.md\"), String))\n\"\"\")","category":"page"}]
}
